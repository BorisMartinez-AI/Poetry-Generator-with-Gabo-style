{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e490ecd1-0b57-4cc8-8f38-250acfec88b3",
   "metadata": {},
   "source": [
    "# Text prediction at Gabo style using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0128ae3-d63d-4156-8809-065da1cd579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library loading\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56da10c5-60af-48b3-ab96-cd6b39673a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 lines look like this:\n",
      "\n",
      "muchos años después, frente al pelotón de fusilamiento, el coronel\n",
      "aureliano buendía había de recordar aquella tarde remota en que su\n",
      "padre lo llevó a conocer el hielo. macondo era entonces una aldea de\n",
      "veinte casas de barro y cañabrava construidas a la orilla de un río de\n",
      "aguas diáfanas que se precipitaban por un lecho de piedras pulidas,\n"
     ]
    }
   ],
   "source": [
    "# Define path for file with sonnets\n",
    "GABO_FILE = './gabo.txt'\n",
    "\n",
    "# Read the data\n",
    "with open(\"./gabo.txt\", encoding = \"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Convert to lower case and save as a list\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "print(f\"The first 5 lines look like this:\\n\")\n",
    "for i in range(5):\n",
    "  print(corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec01bb3-3bdd-4566-860e-01fe6ca53098",
   "metadata": {
    "id": "imB15zrSNhA1"
   },
   "source": [
    "## Tokenizing the text\n",
    "\n",
    "Now fit the Tokenizer to the corpus and save the total number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7116a075-033f-41c6-b85d-28d829a43b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "049be9df-5152-4133-a3fe-f2fad2a67290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'muchos años después, frente al pelotón de fusilamiento, el coronel'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae63387a-aa9c-472e-8e80-3ade05130ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[221, 64, 50, 158, 22, 536, 1, 508, 5, 41]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([corpus[0]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f4f33-d075-4cca-bf5c-61c1232876dc",
   "metadata": {
    "id": "-oqy9KjXRJ9A"
   },
   "source": [
    "## Generating n_grams\n",
    "\n",
    "The `n_gram_seqs` receives the fitted tokenizer and the corpus (which is a list of strings) and should return a list containing the `n_gram` sequences for each line in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482abc1a-06a0-49fe-a352-177d87afec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_seqs(corpus, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a list of n-gram sequences\n",
    "    \n",
    "    Args:\n",
    "        corpus (list of string): lines of texts to generate n-grams for\n",
    "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
    "    \n",
    "    Returns:\n",
    "        input_sequences (list of int): the n-gram sequences for each line in the corpus\n",
    "    \"\"\"\n",
    "    input_sequences = []\n",
    "    \n",
    "    # Loop over every line\n",
    "    for line in corpus:\n",
    "\n",
    "\t    # Tokenize the current line\n",
    "\t    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "\t    # Loop over the line several times to generate the subphrases\n",
    "\t    for i in range(1, len(token_list)):\n",
    "\t\t\n",
    "\t\t    # Generate the subphrase\n",
    "\t\t    n_gram_sequence = token_list[:i+1]\n",
    "\n",
    "\t\t    # Append the subphrase to the sequences list\n",
    "\t\t    input_sequences.append(n_gram_sequence)\n",
    "            \n",
    "    return input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cce6301-0eb8-4006-9644-faab77b7f481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram sequences for first example look like this:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[221, 64],\n",
       " [221, 64, 50],\n",
       " [221, 64, 50, 158],\n",
       " [221, 64, 50, 158, 22],\n",
       " [221, 64, 50, 158, 22, 536],\n",
       " [221, 64, 50, 158, 22, 536, 1],\n",
       " [221, 64, 50, 158, 22, 536, 1, 508],\n",
       " [221, 64, 50, 158, 22, 536, 1, 508, 5],\n",
       " [221, 64, 50, 158, 22, 536, 1, 508, 5, 41]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with one example\n",
    "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
    "\n",
    "print(\"n_gram sequences for first example look like this:\\n\")\n",
    "first_example_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9203683f-71e7-4bb2-a66f-6df904c774d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram sequences for next 3 examples look like this:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[23, 36],\n",
       " [23, 36, 20],\n",
       " [23, 36, 20, 1],\n",
       " [23, 36, 20, 1, 561],\n",
       " [23, 36, 20, 1, 561, 104],\n",
       " [23, 36, 20, 1, 561, 104, 74],\n",
       " [23, 36, 20, 1, 561, 104, 74, 996],\n",
       " [23, 36, 20, 1, 561, 104, 74, 996, 6],\n",
       " [23, 36, 20, 1, 561, 104, 74, 996, 6, 3],\n",
       " [23, 36, 20, 1, 561, 104, 74, 996, 6, 3, 15],\n",
       " [95, 21],\n",
       " [95, 21, 188],\n",
       " [95, 21, 188, 7],\n",
       " [95, 21, 188, 7, 430],\n",
       " [95, 21, 188, 7, 430, 5],\n",
       " [95, 21, 188, 7, 430, 5, 412],\n",
       " [95, 21, 188, 7, 430, 5, 412, 66],\n",
       " [95, 21, 188, 7, 430, 5, 412, 66, 26],\n",
       " [95, 21, 188, 7, 430, 5, 412, 66, 26, 46],\n",
       " [95, 21, 188, 7, 430, 5, 412, 66, 26, 46, 13],\n",
       " [95, 21, 188, 7, 430, 5, 412, 66, 26, 46, 13, 357],\n",
       " [95, 21, 188, 7, 430, 5, 412, 66, 26, 46, 13, 357, 1],\n",
       " [365, 334],\n",
       " [365, 334, 1],\n",
       " [365, 334, 1, 1719],\n",
       " [365, 334, 1, 1719, 4],\n",
       " [365, 334, 1, 1719, 4, 4993],\n",
       " [365, 334, 1, 1719, 4, 4993, 4994],\n",
       " [365, 334, 1, 1719, 4, 4993, 4994, 7],\n",
       " [365, 334, 1, 1719, 4, 4993, 4994, 7, 2],\n",
       " [365, 334, 1, 1719, 4, 4993, 4994, 7, 2, 2902],\n",
       " [365, 334, 1, 1719, 4, 4993, 4994, 7, 2, 2902, 1],\n",
       " [365, 334, 1, 1719, 4, 4993, 4994, 7, 2, 2902, 1, 11],\n",
       " [365, 334, 1, 1719, 4, 4993, 4994, 7, 2, 2902, 1, 11, 701],\n",
       " [365, 334, 1, 1719, 4, 4993, 4994, 7, 2, 2902, 1, 11, 701, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with a bigger corpus\n",
    "next_3_examples_sequence = n_gram_seqs(corpus[1:4], tokenizer)\n",
    "\n",
    "print(\"n_gram sequences for next 3 examples look like this:\\n\")\n",
    "next_3_examples_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff26c97a-a3aa-4773-a413-942f5f2af3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_grams of input_sequences have length: 126015\n",
      "maximum length of sequences is: 27\n"
     ]
    }
   ],
   "source": [
    "# Apply the n_gram_seqs transformation to the whole corpus\n",
    "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
    "\n",
    "# Save max length \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
    "print(f\"maximum length of sequences is: {max_sequence_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb851f6-ed10-4aee-a14a-e9b710ff1546",
   "metadata": {
    "id": "zHY7HroqWq12"
   },
   "source": [
    "## Add padding to the sequences\n",
    "\n",
    "Now the `pad_seqs` function pad any given sequences to the desired maximum length. Notice that this function receives a list of sequences and should return a numpy array with the padded sequences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d9403b4-a263-4aef-928b-8ace1bc6f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seqs(input_sequences, maxlen):\n",
    "    \"\"\"\n",
    "    Pads tokenized sequences to the same length\n",
    "    \n",
    "    Args:\n",
    "        input_sequences (list of int): tokenized sequences to pad\n",
    "        maxlen (int): maximum length of the token sequences\n",
    "    \n",
    "    Returns:\n",
    "        padded_sequences (array of int): tokenized sequences padded to the same length\n",
    "    \"\"\"\n",
    "    padded_sequences = np.array(pad_sequences(input_sequences, maxlen=maxlen, padding='pre'))\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17476108-d3c6-4e05-9ef5-9881d3e3c866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0, 221,  64],\n",
       "       [  0,   0,   0,   0,   0,   0, 221,  64,  50],\n",
       "       [  0,   0,   0,   0,   0, 221,  64,  50, 158],\n",
       "       [  0,   0,   0,   0, 221,  64,  50, 158,  22],\n",
       "       [  0,   0,   0, 221,  64,  50, 158,  22, 536],\n",
       "       [  0,   0, 221,  64,  50, 158,  22, 536,   1],\n",
       "       [  0, 221,  64,  50, 158,  22, 536,   1, 508],\n",
       "       [221,  64,  50, 158,  22, 536,   1, 508,   5],\n",
       "       [ 64,  50, 158,  22, 536,   1, 508,   5,  41]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with the n_grams_seq of the first example\n",
    "first_padded_seq = pad_seqs(first_example_sequence, len(first_example_sequence))\n",
    "first_padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57ad84c9-87dd-4376-82c3-b3f2dd229eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[221, 64],\n",
       " [221, 64, 50],\n",
       " [221, 64, 50, 158],\n",
       " [221, 64, 50, 158, 22],\n",
       " [221, 64, 50, 158, 22, 536],\n",
       " [221, 64, 50, 158, 22, 536, 1],\n",
       " [221, 64, 50, 158, 22, 536, 1, 508],\n",
       " [221, 64, 50, 158, 22, 536, 1, 508, 5],\n",
       " [221, 64, 50, 158, 22, 536, 1, 508, 5, 41]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_example_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa790bb-e1d0-4230-9516-f09c5f216f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,   23,   36],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          23,   36,   20],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   23,\n",
       "          36,   20,    1],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,   23,   36,\n",
       "          20,    1,  561],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,   23,   36,   20,\n",
       "           1,  561,  104],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,   23,   36,   20,    1,\n",
       "         561,  104,   74],\n",
       "       [   0,    0,    0,    0,    0,    0,   23,   36,   20,    1,  561,\n",
       "         104,   74,  996],\n",
       "       [   0,    0,    0,    0,    0,   23,   36,   20,    1,  561,  104,\n",
       "          74,  996,    6],\n",
       "       [   0,    0,    0,    0,   23,   36,   20,    1,  561,  104,   74,\n",
       "         996,    6,    3],\n",
       "       [   0,    0,    0,   23,   36,   20,    1,  561,  104,   74,  996,\n",
       "           6,    3,   15],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,   95,   21],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          95,   21,  188],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   95,\n",
       "          21,  188,    7],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,   95,   21,\n",
       "         188,    7,  430],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,   95,   21,  188,\n",
       "           7,  430,    5],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,   95,   21,  188,    7,\n",
       "         430,    5,  412],\n",
       "       [   0,    0,    0,    0,    0,    0,   95,   21,  188,    7,  430,\n",
       "           5,  412,   66],\n",
       "       [   0,    0,    0,    0,    0,   95,   21,  188,    7,  430,    5,\n",
       "         412,   66,   26],\n",
       "       [   0,    0,    0,    0,   95,   21,  188,    7,  430,    5,  412,\n",
       "          66,   26,   46],\n",
       "       [   0,    0,    0,   95,   21,  188,    7,  430,    5,  412,   66,\n",
       "          26,   46,   13],\n",
       "       [   0,    0,   95,   21,  188,    7,  430,    5,  412,   66,   26,\n",
       "          46,   13,  357],\n",
       "       [   0,   95,   21,  188,    7,  430,    5,  412,   66,   26,   46,\n",
       "          13,  357,    1],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,  365,  334],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         365,  334,    1],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  365,\n",
       "         334,    1, 1719],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,  365,  334,\n",
       "           1, 1719,    4],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,  365,  334,    1,\n",
       "        1719,    4, 4993],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,  365,  334,    1, 1719,\n",
       "           4, 4993, 4994],\n",
       "       [   0,    0,    0,    0,    0,    0,  365,  334,    1, 1719,    4,\n",
       "        4993, 4994,    7],\n",
       "       [   0,    0,    0,    0,    0,  365,  334,    1, 1719,    4, 4993,\n",
       "        4994,    7,    2],\n",
       "       [   0,    0,    0,    0,  365,  334,    1, 1719,    4, 4993, 4994,\n",
       "           7,    2, 2902],\n",
       "       [   0,    0,    0,  365,  334,    1, 1719,    4, 4993, 4994,    7,\n",
       "           2, 2902,    1],\n",
       "       [   0,    0,  365,  334,    1, 1719,    4, 4993, 4994,    7,    2,\n",
       "        2902,    1,   11],\n",
       "       [   0,  365,  334,    1, 1719,    4, 4993, 4994,    7,    2, 2902,\n",
       "           1,   11,  701],\n",
       "       [ 365,  334,    1, 1719,    4, 4993, 4994,    7,    2, 2902,    1,\n",
       "          11,  701,    1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with the n_grams_seq of the next 3 examples\n",
    "next_3_padded_seq = pad_seqs(next_3_examples_sequence, max([len(s) for s in next_3_examples_sequence]))\n",
    "next_3_padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "230f9fc5-8464-4702-92dd-b8509062c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded corpus has shape: (126015, 27)\n"
     ]
    }
   ],
   "source": [
    "# Pad the whole corpus\n",
    "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
    "\n",
    "print(f\"padded corpus has shape: {input_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355757be-7646-446f-87b4-d0d03ddf2126",
   "metadata": {
    "id": "ZbOidyPrXxf7"
   },
   "source": [
    "## Split the data into features and labels\n",
    "\n",
    "Before feeding the data into the neural network you should split it into features and labels. In this case the features will be the padded n_gram sequences with the last word removed from them and the labels will be the removed word.\n",
    "\n",
    "Function `features_and_labels` expects the padded n_gram sequences as input and should return a tuple containing the features and the one hot encoded labels.\n",
    "\n",
    "Notice that the function also receives the total of words in the corpus, this parameter will be very important when one hot enconding the labels since every word in the corpus will be a label at least once. If you need a refresh of how the `to_categorical` function works take a look at the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e6eec9e-0d23-4d71-bf4c-7046d23f501b",
   "metadata": {
    "cellView": "code",
    "id": "9WGGbYdnZdmJ",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def features_and_labels(input_sequences, total_words):\n",
    "    \"\"\"\n",
    "    Generates features and labels from n-grams\n",
    "    \n",
    "    Args:\n",
    "        input_sequences (list of int): sequences to split features and labels from\n",
    "        total_words (int): vocabulary size\n",
    "    \n",
    "    Returns:\n",
    "        features, one_hot_labels (array of int, array of int): arrays of features and one-hot encoded labels\n",
    "    \"\"\"\n",
    "    \n",
    "    features = input_sequences[:,:-1]\n",
    "    labels = input_sequences[:,-1]\n",
    "    one_hot_labels = to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "    return features, one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d1a212c-1a07-4ee4-bda0-5821a2805545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels have shape: (9, 15831)\n",
      "\n",
      "features look like this:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0, 221],\n",
       "       [  0,   0,   0,   0,   0,   0, 221,  64],\n",
       "       [  0,   0,   0,   0,   0, 221,  64,  50],\n",
       "       [  0,   0,   0,   0, 221,  64,  50, 158],\n",
       "       [  0,   0,   0, 221,  64,  50, 158,  22],\n",
       "       [  0,   0, 221,  64,  50, 158,  22, 536],\n",
       "       [  0, 221,  64,  50, 158,  22, 536,   1],\n",
       "       [221,  64,  50, 158,  22, 536,   1, 508],\n",
       "       [ 64,  50, 158,  22, 536,   1, 508,   5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with the padded n_grams_seq of the first example\n",
    "first_features, first_labels = features_and_labels(first_padded_seq, total_words)\n",
    "\n",
    "print(f\"labels have shape: {first_labels.shape}\")\n",
    "print(\"\\nfeatures look like this:\\n\")\n",
    "first_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f39cac70-8676-4aea-aab9-3384dca090e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features have shape: (126015, 26)\n",
      "labels have shape: (126015, 15831)\n"
     ]
    }
   ],
   "source": [
    "# Split the whole corpus\n",
    "features, labels = features_and_labels(input_sequences, total_words)\n",
    "\n",
    "print(f\"features have shape: {features.shape}\")\n",
    "print(f\"labels have shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3da99b-4a2b-4c84-9abc-4cdda31f7a15",
   "metadata": {
    "id": "ltxaOCE_aU6J"
   },
   "source": [
    "## Create the model\n",
    "\n",
    "Now you should define a model architecture capable of achieving an accuracy of at least 80%.\n",
    "\n",
    "Some hints to help you in this task:\n",
    "\n",
    "- An appropriate `output_dim` for the first layer (Embedding) is 100, this is already provided for you.\n",
    "- A Bidirectional LSTM is helpful for this particular problem.\n",
    "- The last layer should have the same number of units as the total number of words in the corpus and a softmax activation function.\n",
    "- This problem can be solved with only two layers (excluding the Embedding) so try out small architectures first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af1ba179-72f8-4030-a097-92a120dae812",
   "metadata": {
    "cellView": "code",
    "id": "XrE6kpJFfvRY",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def create_model(total_words, max_sequence_len):\n",
    "    \"\"\"\n",
    "    Creates a text generator model\n",
    "    \n",
    "    Args:\n",
    "        total_words (int): size of the vocabulary for the Embedding layer input\n",
    "        max_sequence_len (int): length of the input sequences\n",
    "    \n",
    "    Returns:\n",
    "        model (tf.keras Model): the text generator model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(Bidirectional(LSTM(100)))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer= 'Adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58cafa4a-2682-488d-bf93-4d82f4a9f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3938/3938 [==============================] - 400s 97ms/step - loss: 6.5605 - accuracy: 0.0903\n",
      "Epoch 2/50\n",
      "3938/3938 [==============================] - 401s 102ms/step - loss: 5.9061 - accuracy: 0.1147\n",
      "Epoch 3/50\n",
      "3938/3938 [==============================] - 428s 109ms/step - loss: 5.5752 - accuracy: 0.1365\n",
      "Epoch 4/50\n",
      "3938/3938 [==============================] - 436s 111ms/step - loss: 5.3243 - accuracy: 0.1557\n",
      "Epoch 5/50\n",
      "3938/3938 [==============================] - 405s 103ms/step - loss: 5.0922 - accuracy: 0.1746\n",
      "Epoch 6/50\n",
      "3938/3938 [==============================] - 409s 104ms/step - loss: 4.8852 - accuracy: 0.1887\n",
      "Epoch 7/50\n",
      "3938/3938 [==============================] - 426s 108ms/step - loss: 4.7059 - accuracy: 0.1982\n",
      "Epoch 8/50\n",
      "3938/3938 [==============================] - 451s 115ms/step - loss: 4.5411 - accuracy: 0.2079\n",
      "Epoch 9/50\n",
      "3938/3938 [==============================] - 462s 117ms/step - loss: 4.3804 - accuracy: 0.2187\n",
      "Epoch 10/50\n",
      "3938/3938 [==============================] - 432s 110ms/step - loss: 4.2264 - accuracy: 0.2263\n",
      "Epoch 11/50\n",
      "3938/3938 [==============================] - 437s 111ms/step - loss: 4.0703 - accuracy: 0.2359\n",
      "Epoch 12/50\n",
      "3938/3938 [==============================] - 470s 119ms/step - loss: 3.9164 - accuracy: 0.2454\n",
      "Epoch 13/50\n",
      "3938/3938 [==============================] - 483s 123ms/step - loss: 3.7578 - accuracy: 0.2588\n",
      "Epoch 14/50\n",
      "3938/3938 [==============================] - 479s 122ms/step - loss: 3.6014 - accuracy: 0.2730\n",
      "Epoch 15/50\n",
      "3938/3938 [==============================] - 467s 119ms/step - loss: 3.4509 - accuracy: 0.2897\n",
      "Epoch 16/50\n",
      "3938/3938 [==============================] - 481s 122ms/step - loss: 3.2988 - accuracy: 0.3090\n",
      "Epoch 17/50\n",
      "3938/3938 [==============================] - 482s 122ms/step - loss: 3.1522 - accuracy: 0.3284\n",
      "Epoch 18/50\n",
      "3938/3938 [==============================] - 492s 125ms/step - loss: 3.0055 - accuracy: 0.3511\n",
      "Epoch 19/50\n",
      "3938/3938 [==============================] - 521s 132ms/step - loss: 2.8579 - accuracy: 0.3736\n",
      "Epoch 20/50\n",
      "3938/3938 [==============================] - 545s 138ms/step - loss: 2.7114 - accuracy: 0.3989\n",
      "Epoch 21/50\n",
      "3938/3938 [==============================] - 549s 139ms/step - loss: 2.5725 - accuracy: 0.4232\n",
      "Epoch 22/50\n",
      "3938/3938 [==============================] - 531s 135ms/step - loss: 2.4371 - accuracy: 0.4491\n",
      "Epoch 23/50\n",
      "3938/3938 [==============================] - 542s 138ms/step - loss: 2.3054 - accuracy: 0.4743\n",
      "Epoch 24/50\n",
      "3938/3938 [==============================] - 550s 140ms/step - loss: 2.1817 - accuracy: 0.4982\n",
      "Epoch 25/50\n",
      "3938/3938 [==============================] - 543s 138ms/step - loss: 2.0686 - accuracy: 0.5210\n",
      "Epoch 26/50\n",
      "3938/3938 [==============================] - 544s 138ms/step - loss: 1.9589 - accuracy: 0.5445\n",
      "Epoch 27/50\n",
      "3938/3938 [==============================] - 542s 138ms/step - loss: 1.8597 - accuracy: 0.5651\n",
      "Epoch 28/50\n",
      "3938/3938 [==============================] - 550s 140ms/step - loss: 1.7564 - accuracy: 0.5866\n",
      "Epoch 29/50\n",
      "3938/3938 [==============================] - 574s 146ms/step - loss: 1.6636 - accuracy: 0.6082\n",
      "Epoch 30/50\n",
      "3938/3938 [==============================] - 579s 147ms/step - loss: 1.5757 - accuracy: 0.6266\n",
      "Epoch 31/50\n",
      "3938/3938 [==============================] - 585s 148ms/step - loss: 1.4914 - accuracy: 0.6450\n",
      "Epoch 32/50\n",
      "3938/3938 [==============================] - 592s 150ms/step - loss: 1.4170 - accuracy: 0.6611\n",
      "Epoch 33/50\n",
      "3938/3938 [==============================] - 575s 146ms/step - loss: 1.3434 - accuracy: 0.6789\n",
      "Epoch 34/50\n",
      "3938/3938 [==============================] - 593s 151ms/step - loss: 1.2804 - accuracy: 0.6929\n",
      "Epoch 35/50\n",
      "3938/3938 [==============================] - 597s 152ms/step - loss: 1.2178 - accuracy: 0.7077\n",
      "Epoch 36/50\n",
      "3938/3938 [==============================] - 615s 156ms/step - loss: 1.1625 - accuracy: 0.7206\n",
      "Epoch 37/50\n",
      "3938/3938 [==============================] - 645s 164ms/step - loss: 1.1066 - accuracy: 0.7333\n",
      "Epoch 38/50\n",
      "3938/3938 [==============================] - 625s 159ms/step - loss: 1.0568 - accuracy: 0.7448\n",
      "Epoch 39/50\n",
      "3938/3938 [==============================] - 616s 157ms/step - loss: 1.0130 - accuracy: 0.7537\n",
      "Epoch 40/50\n",
      "3938/3938 [==============================] - 627s 159ms/step - loss: 0.9705 - accuracy: 0.7636\n",
      "Epoch 41/50\n",
      "3938/3938 [==============================] - 639s 162ms/step - loss: 0.9320 - accuracy: 0.7735\n",
      "Epoch 42/50\n",
      "3938/3938 [==============================] - 633s 161ms/step - loss: 0.8982 - accuracy: 0.7804\n",
      "Epoch 43/50\n",
      "3938/3938 [==============================] - 662s 168ms/step - loss: 0.8600 - accuracy: 0.7894\n",
      "Epoch 44/50\n",
      "3938/3938 [==============================] - 652s 166ms/step - loss: 0.8311 - accuracy: 0.7959\n",
      "Epoch 45/50\n",
      "3938/3938 [==============================] - 655s 166ms/step - loss: 0.8032 - accuracy: 0.8026\n",
      "Epoch 46/50\n",
      "3938/3938 [==============================] - 664s 169ms/step - loss: 0.7755 - accuracy: 0.8105\n",
      "Epoch 47/50\n",
      "3938/3938 [==============================] - 675s 171ms/step - loss: 0.7479 - accuracy: 0.8155\n",
      "Epoch 48/50\n",
      "3938/3938 [==============================] - 675s 172ms/step - loss: 0.7262 - accuracy: 0.8197\n",
      "Epoch 49/50\n",
      "3938/3938 [==============================] - 688s 175ms/step - loss: 0.7039 - accuracy: 0.8261\n",
      "Epoch 50/50\n",
      "3938/3938 [==============================] - 700s 178ms/step - loss: 0.6882 - accuracy: 0.8289\n"
     ]
    }
   ],
   "source": [
    "# Get the untrained model\n",
    "model = create_model(total_words, max_sequence_len)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(features, labels, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb37d9b9-650f-4fe2-bf26-aaa8442422e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando el modelo\n",
    "model.save(\"gabo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0543af0e-50ec-4781-a2c3-b4dac45b757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 26, 100)           1583100   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 26, 200)          160800    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 200)              240800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15831)             3182031   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,166,731\n",
      "Trainable params: 5,166,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model('gabo.h5')\n",
    "# print summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa0d3e74-5e0c-48b8-9182-28a1cea97af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3938/3938 [==============================] - 403s 98ms/step - loss: 0.4731 - accuracy: 0.8774\n",
      "Epoch 2/20\n",
      "3938/3938 [==============================] - 401s 102ms/step - loss: 0.4669 - accuracy: 0.8792\n",
      "Epoch 3/20\n",
      "3938/3938 [==============================] - 422s 107ms/step - loss: 0.4678 - accuracy: 0.8782\n",
      "Epoch 4/20\n",
      "3938/3938 [==============================] - 434s 110ms/step - loss: 0.4567 - accuracy: 0.8801\n",
      "Epoch 5/20\n",
      "3938/3938 [==============================] - 422s 107ms/step - loss: 0.4553 - accuracy: 0.8816\n",
      "Epoch 6/20\n",
      "3938/3938 [==============================] - 419s 106ms/step - loss: 0.4529 - accuracy: 0.8817\n",
      "Epoch 7/20\n",
      "3938/3938 [==============================] - 417s 106ms/step - loss: 0.4505 - accuracy: 0.8821\n",
      "Epoch 8/20\n",
      "3938/3938 [==============================] - 409s 104ms/step - loss: 0.4436 - accuracy: 0.8841\n",
      "Epoch 9/20\n",
      "3938/3938 [==============================] - 428s 109ms/step - loss: 0.4436 - accuracy: 0.8845\n",
      "Epoch 10/20\n",
      "3938/3938 [==============================] - 613s 156ms/step - loss: 0.4391 - accuracy: 0.8842\n",
      "Epoch 11/20\n",
      "3938/3938 [==============================] - 422s 107ms/step - loss: 0.4353 - accuracy: 0.8851\n",
      "Epoch 12/20\n",
      "3938/3938 [==============================] - 601s 153ms/step - loss: 0.4322 - accuracy: 0.8864\n",
      "Epoch 13/20\n",
      "3938/3938 [==============================] - 473s 120ms/step - loss: 0.4326 - accuracy: 0.8862\n",
      "Epoch 14/20\n",
      "3938/3938 [==============================] - 623s 158ms/step - loss: 0.4305 - accuracy: 0.8858\n",
      "Epoch 15/20\n",
      "3938/3938 [==============================] - 464s 118ms/step - loss: 0.4276 - accuracy: 0.8872\n",
      "Epoch 16/20\n",
      "3938/3938 [==============================] - 461s 117ms/step - loss: 0.4212 - accuracy: 0.8886\n",
      "Epoch 17/20\n",
      "3938/3938 [==============================] - 484s 123ms/step - loss: 0.4219 - accuracy: 0.8884\n",
      "Epoch 18/20\n",
      "3938/3938 [==============================] - 515s 131ms/step - loss: 0.4230 - accuracy: 0.8872\n",
      "Epoch 19/20\n",
      "3938/3938 [==============================] - 517s 131ms/step - loss: 0.4185 - accuracy: 0.8889\n",
      "Epoch 20/20\n",
      "3938/3938 [==============================] - 547s 139ms/step - loss: 0.4128 - accuracy: 0.8901\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(features, labels, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19c12ec3-2c17-4d00-b464-bdd418e93fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macondo en el estrecho cuartito atiborrado de frascos vacíos que alquiló el día siguiente de sus súplicas y los baúles en brazos y un llanto de regreso al cuarto de melquíades estaba casi por la orden a la calle y el sueño y perdido el nigromante se lo había hecho el más leve suspiro de risa y que la familia y la oyó un par de medias en el mollera hasta el día en que importaba la tarde en que el fabricarse y varios siglos frente a la la vida desde la vida santa sofía de la piedad » lo vio\n"
     ]
    }
   ],
   "source": [
    "#Text generation\n",
    "seed_text = \"macondo\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\t# Convert the text into sequences\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\t# Pad the sequences\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\t# Get the probabilities of predicting a word\n",
    "\tpredicted = model.predict(token_list, verbose=0)\n",
    "\t# Choose the next word based on the maximum probability\n",
    "\tpredicted = np.argmax(predicted, axis=-1).item()\n",
    "\t# Get the actual word from the word index\n",
    "\toutput_word = tokenizer.index_word[predicted]\n",
    "\t# Append to the current text\n",
    "\tseed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e670e7-2866-4681-83ca-0bc846542731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually adding punctuation and making some cosmetic changes to give it more sense to the sentences with a touch of poetry!\n",
    "\"\"\"\n",
    "\n",
    "Macondo en el estrecho cuartito atiborrado de frascos vacíos que alquiló al día siguiente de sus súplicas. Los baúles en brazos y un llanto de regreso al cuarto de Melquíades que estaba por \n",
    "la calle y el sueño perdido del nigromante se había hecho el más leve suspiro de risa que la familia oyó, hasta el día en que importaba la tarde de varios siglos frente a la vida de \n",
    "Santa Sofía de la piedad ...\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
